{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "106d11ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "加载分词器: hfl/chinese-roberta-wwm-ext\n",
      "正在加载和处理数据...\n",
      "原始数据量: 995\n",
      "去重后数据量: 985\n",
      "删除标记为删除的行后数据量: 964 (删除了 21 行)\n",
      "数据已随机排序\n",
      "歧义句数量: 963\n",
      "消歧句数量: 1926\n",
      "总数据量: 2889\n",
      "\n",
      "=== 整体数据 数据分布 ===\n",
      "总数量: 2889\n",
      "歧义句: 963 (33.33%)\n",
      "非歧义句: 1926 (66.67%)\n",
      "\n",
      "各类型分布:\n",
      "  词汇-兼类词: 48 (1.66%)\n",
      "  词汇-同形词: 29 (1.00%)\n",
      "  词汇-多义词: 174 (6.02%)\n",
      "  语法-句法歧义: 270 (9.35%)\n",
      "  语法-语义歧义: 70 (2.42%)\n",
      "  语用语境-会话含义: 83 (2.87%)\n",
      "  语用语境-指示语: 51 (1.77%)\n",
      "  语用语境-社会文化: 129 (4.47%)\n",
      "  语用语境-言语行为: 109 (3.77%)\n",
      "  非歧义-词汇-兼类词: 96 (3.32%)\n",
      "  非歧义-词汇-同形词: 58 (2.01%)\n",
      "  非歧义-词汇-多义词: 348 (12.05%)\n",
      "  非歧义-语法-句法歧义: 540 (18.69%)\n",
      "  非歧义-语法-语义歧义: 140 (4.85%)\n",
      "  非歧义-语用语境-会话含义: 166 (5.75%)\n",
      "  非歧义-语用语境-指示语: 102 (3.53%)\n",
      "  非歧义-语用语境-社会文化: 258 (8.93%)\n",
      "  非歧义-语用语境-言语行为: 218 (7.55%)\n",
      "\n",
      "分层标签分布:\n",
      "  0_非歧义-词汇-兼类词: 96\n",
      "  0_非歧义-词汇-同形词: 58\n",
      "  0_非歧义-词汇-多义词: 348\n",
      "  0_非歧义-语法-句法歧义: 540\n",
      "  0_非歧义-语法-语义歧义: 140\n",
      "  0_非歧义-语用语境-会话含义: 166\n",
      "  0_非歧义-语用语境-指示语: 102\n",
      "  0_非歧义-语用语境-社会文化: 258\n",
      "  0_非歧义-语用语境-言语行为: 218\n",
      "  1_词汇-兼类词: 48\n",
      "  1_词汇-同形词: 29\n",
      "  1_词汇-多义词: 174\n",
      "  1_语法-句法歧义: 270\n",
      "  1_语法-语义歧义: 70\n",
      "  1_语用语境-会话含义: 83\n",
      "  1_语用语境-指示语: 51\n",
      "  1_语用语境-社会文化: 129\n",
      "  1_语用语境-言语行为: 109\n",
      "\n",
      "=== 数据集划分结果 ===\n",
      "训练集大小: 2022 (70.0%)\n",
      "验证集大小: 433 (15.0%)\n",
      "测试集大小: 434 (15.0%)\n",
      "\n",
      "=== 训练集 数据分布 ===\n",
      "总数量: 2022\n",
      "歧义句: 674 (33.33%)\n",
      "非歧义句: 1348 (66.67%)\n",
      "\n",
      "各类型分布:\n",
      "  词汇-兼类词: 34 (1.68%)\n",
      "  词汇-同形词: 20 (0.99%)\n",
      "  词汇-多义词: 122 (6.03%)\n",
      "  语法-句法歧义: 189 (9.35%)\n",
      "  语法-语义歧义: 49 (2.42%)\n",
      "  语用语境-会话含义: 58 (2.87%)\n",
      "  语用语境-指示语: 36 (1.78%)\n",
      "  语用语境-社会文化: 90 (4.45%)\n",
      "  语用语境-言语行为: 76 (3.76%)\n",
      "  非歧义-词汇-兼类词: 67 (3.31%)\n",
      "  非歧义-词汇-同形词: 41 (2.03%)\n",
      "  非歧义-词汇-多义词: 243 (12.02%)\n",
      "  非歧义-语法-句法歧义: 378 (18.69%)\n",
      "  非歧义-语法-语义歧义: 98 (4.85%)\n",
      "  非歧义-语用语境-会话含义: 116 (5.74%)\n",
      "  非歧义-语用语境-指示语: 71 (3.51%)\n",
      "  非歧义-语用语境-社会文化: 181 (8.95%)\n",
      "  非歧义-语用语境-言语行为: 153 (7.57%)\n",
      "\n",
      "=== 验证集 数据分布 ===\n",
      "总数量: 433\n",
      "歧义句: 145 (33.49%)\n",
      "非歧义句: 288 (66.51%)\n",
      "\n",
      "各类型分布:\n",
      "  词汇-兼类词: 7 (1.62%)\n",
      "  词汇-同形词: 5 (1.15%)\n",
      "  词汇-多义词: 26 (6.00%)\n",
      "  语法-句法歧义: 40 (9.24%)\n",
      "  语法-语义歧义: 11 (2.54%)\n",
      "  语用语境-会话含义: 13 (3.00%)\n",
      "  语用语境-指示语: 8 (1.85%)\n",
      "  语用语境-社会文化: 19 (4.39%)\n",
      "  语用语境-言语行为: 16 (3.70%)\n",
      "  非歧义-词汇-兼类词: 15 (3.46%)\n",
      "  非歧义-词汇-同形词: 9 (2.08%)\n",
      "  非歧义-词汇-多义词: 52 (12.01%)\n",
      "  非歧义-语法-句法歧义: 81 (18.71%)\n",
      "  非歧义-语法-语义歧义: 21 (4.85%)\n",
      "  非歧义-语用语境-会话含义: 25 (5.77%)\n",
      "  非歧义-语用语境-指示语: 15 (3.46%)\n",
      "  非歧义-语用语境-社会文化: 38 (8.78%)\n",
      "  非歧义-语用语境-言语行为: 32 (7.39%)\n",
      "\n",
      "=== 测试集 数据分布 ===\n",
      "总数量: 434\n",
      "歧义句: 144 (33.18%)\n",
      "非歧义句: 290 (66.82%)\n",
      "\n",
      "各类型分布:\n",
      "  词汇-兼类词: 7 (1.61%)\n",
      "  词汇-同形词: 4 (0.92%)\n",
      "  词汇-多义词: 26 (5.99%)\n",
      "  语法-句法歧义: 41 (9.45%)\n",
      "  语法-语义歧义: 10 (2.30%)\n",
      "  语用语境-会话含义: 12 (2.76%)\n",
      "  语用语境-指示语: 7 (1.61%)\n",
      "  语用语境-社会文化: 20 (4.61%)\n",
      "  语用语境-言语行为: 17 (3.92%)\n",
      "  非歧义-词汇-兼类词: 14 (3.23%)\n",
      "  非歧义-词汇-同形词: 8 (1.84%)\n",
      "  非歧义-词汇-多义词: 53 (12.21%)\n",
      "  非歧义-语法-句法歧义: 81 (18.66%)\n",
      "  非歧义-语法-语义歧义: 21 (4.84%)\n",
      "  非歧义-语用语境-会话含义: 25 (5.76%)\n",
      "  非歧义-语用语境-指示语: 16 (3.69%)\n",
      "  非歧义-语用语境-社会文化: 39 (8.99%)\n",
      "  非歧义-语用语境-言语行为: 33 (7.60%)\n",
      "\n",
      "=== 分布一致性验证 ===\n",
      "词汇-兼类词:\n",
      "  训练集: 1.68%, 验证集: 1.62%, 测试集: 1.61%\n",
      "词汇-同形词:\n",
      "  训练集: 0.99%, 验证集: 1.15%, 测试集: 0.92%\n",
      "词汇-多义词:\n",
      "  训练集: 6.03%, 验证集: 6.00%, 测试集: 5.99%\n",
      "语法-句法歧义:\n",
      "  训练集: 9.35%, 验证集: 9.24%, 测试集: 9.45%\n",
      "语法-语义歧义:\n",
      "  训练集: 2.42%, 验证集: 2.54%, 测试集: 2.30%\n",
      "语用语境-会话含义:\n",
      "  训练集: 2.87%, 验证集: 3.00%, 测试集: 2.76%\n",
      "语用语境-指示语:\n",
      "  训练集: 1.78%, 验证集: 1.85%, 测试集: 1.61%\n",
      "语用语境-社会文化:\n",
      "  训练集: 4.45%, 验证集: 4.39%, 测试集: 4.61%\n",
      "语用语境-言语行为:\n",
      "  训练集: 3.76%, 验证集: 3.70%, 测试集: 3.92%\n",
      "非歧义-词汇-兼类词:\n",
      "  训练集: 3.31%, 验证集: 3.46%, 测试集: 3.23%\n",
      "非歧义-词汇-同形词:\n",
      "  训练集: 2.03%, 验证集: 2.08%, 测试集: 1.84%\n",
      "非歧义-词汇-多义词:\n",
      "  训练集: 12.02%, 验证集: 12.01%, 测试集: 12.21%\n",
      "非歧义-语法-句法歧义:\n",
      "  训练集: 18.69%, 验证集: 18.71%, 测试集: 18.66%\n",
      "非歧义-语法-语义歧义:\n",
      "  训练集: 4.85%, 验证集: 4.85%, 测试集: 4.84%\n",
      "非歧义-语用语境-会话含义:\n",
      "  训练集: 5.74%, 验证集: 5.77%, 测试集: 5.76%\n",
      "非歧义-语用语境-指示语:\n",
      "  训练集: 3.51%, 验证集: 3.46%, 测试集: 3.69%\n",
      "非歧义-语用语境-社会文化:\n",
      "  训练集: 8.95%, 验证集: 8.78%, 测试集: 8.99%\n",
      "非歧义-语用语境-言语行为:\n",
      "  训练集: 7.57%, 验证集: 7.39%, 测试集: 7.60%\n",
      "已保存 train 数据集到: ./datasets\\train.tsv\n",
      "已保存 val 数据集到: ./datasets\\dev.tsv\n",
      "已保存 test 数据集到: ./datasets\\test.tsv\n",
      "正在创建数据加载器...\n",
      "加载模型: hfl/chinese-roberta-wwm-ext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "开始 Epoch 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:38<00:00,  3.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 平均训练损失: 0.2592\n",
      "Epoch 1 验证集评估:\n",
      "  准确率: 0.9192\n",
      "  精确率: 0.9044\n",
      "  召回率: 0.8483\n",
      "  F1分数: 0.8754\n",
      "开始 Epoch 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:50<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 平均训练损失: 0.1825\n",
      "Epoch 2 验证集评估:\n",
      "  准确率: 0.9169\n",
      "  精确率: 0.9658\n",
      "  召回率: 0.7793\n",
      "  F1分数: 0.8626\n",
      "开始 Epoch 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:50<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 平均训练损失: 0.1471\n",
      "Epoch 3 验证集评估:\n",
      "  准确率: 0.8961\n",
      "  精确率: 0.7747\n",
      "  召回率: 0.9724\n",
      "  F1分数: 0.8624\n",
      "开始 Epoch 4/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:53<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 平均训练损失: 0.0800\n",
      "Epoch 4 验证集评估:\n",
      "  准确率: 0.9284\n",
      "  精确率: 0.9385\n",
      "  召回率: 0.8414\n",
      "  F1分数: 0.8873\n",
      "开始 Epoch 5/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:54<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 平均训练损失: 0.0496\n",
      "Epoch 5 验证集评估:\n",
      "  准确率: 0.9330\n",
      "  精确率: 0.9462\n",
      "  召回率: 0.8483\n",
      "  F1分数: 0.8945\n",
      "开始 Epoch 6/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:55<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 平均训练损失: 0.0293\n",
      "Epoch 6 验证集评估:\n",
      "  准确率: 0.9376\n",
      "  精确率: 0.9538\n",
      "  召回率: 0.8552\n",
      "  F1分数: 0.9018\n",
      "开始 Epoch 7/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:55<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 平均训练损失: 0.0137\n",
      "Epoch 7 验证集评估:\n",
      "  准确率: 0.9492\n",
      "  精确率: 0.9624\n",
      "  召回率: 0.8828\n",
      "  F1分数: 0.9209\n",
      "开始 Epoch 8/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:55<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 平均训练损失: 0.0035\n",
      "Epoch 8 验证集评估:\n",
      "  准确率: 0.9446\n",
      "  精确率: 0.9116\n",
      "  召回率: 0.9241\n",
      "  F1分数: 0.9178\n",
      "开始 Epoch 9/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:55<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 平均训练损失: 0.0027\n",
      "Epoch 9 验证集评估:\n",
      "  准确率: 0.9515\n",
      "  精确率: 0.9493\n",
      "  召回率: 0.9034\n",
      "  F1分数: 0.9258\n",
      "开始 Epoch 10/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中: 100%|██████████| 127/127 [00:55<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 平均训练损失: 0.0016\n",
      "Epoch 10 验证集评估:\n",
      "  准确率: 0.9515\n",
      "  精确率: 0.9493\n",
      "  召回率: 0.9034\n",
      "  F1分数: 0.9258\n",
      "正在对测试集进行预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "预测中: 100%|██████████| 434/434 [00:05<00:00, 73.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集预测结果已保存到: test_predictions.csv\n",
      "测试集结果:\n",
      "  准确率: 0.9470\n",
      "  精确率: 0.9416\n",
      "  召回率: 0.8958\n",
      "  F1分数: 0.9181\n",
      "保存模型到 ./ambiguity_detection_model_FINAL\n",
      "完整流程执行完成！\n",
      "生成的文件:\n",
      "- train.tsv: 训练集\n",
      "- dev.tsv: 验证集\n",
      "- test.tsv: 测试集\n",
      "- test_predictions.csv: 测试集预测结果\n",
      "- ./ambiguity_detection_model_FINAL: 训练好的模型\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    get_linear_schedule_with_warmup,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 数据处理部分\n",
    "class AmbiguityDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def enrich_with_linguistic_features(text):\n",
    "    words = pseg.cut(text)\n",
    "    tokens = []\n",
    "    pos_tags = []\n",
    "    for word, flag in words:\n",
    "        tokens.append(word)\n",
    "        pos_tags.append(flag)\n",
    "    token_str = ' '.join(tokens)\n",
    "    pos_str = ' '.join(pos_tags)\n",
    "    enriched_text = f\"{text} [TOKENS] {token_str} [POS] {pos_str}\"\n",
    "    return enriched_text\n",
    "\n",
    "def print_distribution_stats(labels, types, dataset_name):\n",
    "    \"\"\"打印数据集中各类型的分布统计\"\"\"\n",
    "    print(f\"\\n=== {dataset_name} 数据分布 ===\")\n",
    "    total_count = len(labels)\n",
    "    \n",
    "    # 按歧义/非歧义统计\n",
    "    ambig_count = sum(labels)\n",
    "    non_ambig_count = total_count - ambig_count\n",
    "    print(f\"总数量: {total_count}\")\n",
    "    print(f\"歧义句: {ambig_count} ({ambig_count/total_count*100:.2f}%)\")\n",
    "    print(f\"非歧义句: {non_ambig_count} ({non_ambig_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    # 按类型统计\n",
    "    type_counter = Counter(types)\n",
    "    print(f\"\\n各类型分布:\")\n",
    "    for type_name, count in sorted(type_counter.items()):\n",
    "        percentage = count / total_count * 100\n",
    "        print(f\"  {type_name}: {count} ({percentage:.2f}%)\")\n",
    "    \n",
    "    return type_counter\n",
    "\n",
    "def prepare_data_with_requirements(file_path):\n",
    "    \"\"\"\n",
    "    按照实验要求处理数据：去重、删除不需要的行、随机排序、划分数据集\n",
    "    \"\"\"\n",
    "    print(\"正在加载和处理数据...\")\n",
    "    \n",
    "    # 读取数据集\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"原始数据量: {len(df)}\")\n",
    "    \n",
    "    # 0. 去重和删除标记为删除的行\n",
    "    # 首先基于歧义句去重\n",
    "    df = df.drop_duplicates(subset=['歧义句'], keep='first')\n",
    "    print(f\"去重后数据量: {len(df)}\")\n",
    "    \n",
    "    # 删除【是否删除】列不为空的行\n",
    "    if '是否删除' in df.columns:\n",
    "        before_delete = len(df)\n",
    "        df = df[df['是否删除'].isna()]\n",
    "        print(f\"删除标记为删除的行后数据量: {len(df)} (删除了 {before_delete - len(df)} 行)\")\n",
    "    \n",
    "    # 检查必要的列\n",
    "    required_columns = ['歧义句', '歧义句消岐1', '歧义句消岐2', '歧义类型']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"数据集缺少必要的列: {col}\")\n",
    "    \n",
    "    # 2. 随机排序 (种子为0)\n",
    "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    print(\"数据已随机排序\")\n",
    "    \n",
    "    # 准备训练数据\n",
    "    # 歧义句作为正样本\n",
    "    ambiguous_data = df[df['歧义句'].notna()].copy()\n",
    "    ambiguous_texts = ambiguous_data['歧义句'].apply(enrich_with_linguistic_features).tolist()\n",
    "    ambiguous_labels = [1] * len(ambiguous_texts)\n",
    "    # 歧义句保留原始的歧义类型\n",
    "    ambiguous_types = ambiguous_data['歧义类型'].fillna('未知').tolist()\n",
    "    \n",
    "    # 消歧句作为负样本\n",
    "    disambig_texts = []\n",
    "    disambig_labels = []\n",
    "    disambig_types = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        ambig_type = row['歧义类型'] if pd.notna(row['歧义类型']) else '未知'\n",
    "        for col in ['歧义句消岐1', '歧义句消岐2']:\n",
    "            if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                disambig_texts.append(enrich_with_linguistic_features(str(row[col])))\n",
    "                disambig_labels.append(0)\n",
    "                # 消歧句使用特殊的类型标记，表明它们是对应某种歧义类型的消歧版本\n",
    "                disambig_types.append(f\"非歧义-{ambig_type}\")\n",
    "    \n",
    "    # 合并所有数据\n",
    "    all_texts = ambiguous_texts + disambig_texts\n",
    "    all_labels = ambiguous_labels + disambig_labels\n",
    "    all_types = ambiguous_types + disambig_types\n",
    "    \n",
    "    print(f\"歧义句数量: {len(ambiguous_texts)}\")\n",
    "    print(f\"消歧句数量: {len(disambig_texts)}\")\n",
    "    print(f\"总数据量: {len(all_texts)}\")\n",
    "    \n",
    "    # 打印整体数据分布\n",
    "    print_distribution_stats(all_labels, all_types, \"整体数据\")\n",
    "    \n",
    "    # 3. 划分数据集 (种子为2025)，使用改进的分层策略\n",
    "    # 对于样本数量太少的类别，我们将它们合并到主要类别中进行分层\n",
    "    type_counts = Counter(all_types)\n",
    "    min_samples_for_stratify = 3  # 每个类别至少需要3个样本才能进行分层\n",
    "    \n",
    "    # 创建分层标签\n",
    "    stratify_labels = []\n",
    "    for label, atype in zip(all_labels, all_types):\n",
    "        if type_counts[atype] >= min_samples_for_stratify:\n",
    "            # 样本足够的类别使用完整的分层标签\n",
    "            stratify_labels.append(f\"{label}_{atype}\")\n",
    "        else:\n",
    "            # 样本不足的类别只按歧义/非歧义分层\n",
    "            stratify_labels.append(f\"{label}_其他\")\n",
    "    \n",
    "    # 检查分层标签的分布\n",
    "    stratify_counts = Counter(stratify_labels)\n",
    "    print(f\"\\n分层标签分布:\")\n",
    "    for label, count in sorted(stratify_counts.items()):\n",
    "        print(f\"  {label}: {count}\")\n",
    "    \n",
    "    # 首先划分出训练集 (70%)\n",
    "    train_texts, temp_texts, train_labels, temp_labels, train_types, temp_types, train_strat, temp_strat = train_test_split(\n",
    "        all_texts, all_labels, all_types, stratify_labels,\n",
    "        test_size=0.3, stratify=stratify_labels, random_state=2025\n",
    "    )\n",
    "    \n",
    "    # 计算需要的测试集大小，确保至少100条\n",
    "    min_test_size = 100\n",
    "    temp_size = len(temp_texts)\n",
    "    \n",
    "    if temp_size < min_test_size:\n",
    "        raise ValueError(f\"剩余数据量 ({temp_size}) 不足以创建100条测试集\")\n",
    "    \n",
    "    # 计算测试集比例，确保至少100条\n",
    "    test_ratio = max(0.5, min_test_size / temp_size)  # 在临时数据中的比例\n",
    "    \n",
    "    # 划分验证集和测试集\n",
    "    val_texts, test_texts, val_labels, test_labels, val_types, test_types = train_test_split(\n",
    "        temp_texts, temp_labels, temp_types,\n",
    "        test_size=test_ratio, stratify=temp_strat, random_state=2025\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== 数据集划分结果 ===\")\n",
    "    print(f\"训练集大小: {len(train_texts)} ({len(train_texts)/len(all_texts)*100:.1f}%)\")\n",
    "    print(f\"验证集大小: {len(val_texts)} ({len(val_texts)/len(all_texts)*100:.1f}%)\")\n",
    "    print(f\"测试集大小: {len(test_texts)} ({len(test_texts)/len(all_texts)*100:.1f}%)\")\n",
    "    \n",
    "    # 打印各数据集的详细分布\n",
    "    train_dist = print_distribution_stats(train_labels, train_types, \"训练集\")\n",
    "    val_dist = print_distribution_stats(val_labels, val_types, \"验证集\")  \n",
    "    test_dist = print_distribution_stats(test_labels, test_types, \"测试集\")\n",
    "    \n",
    "    # 验证分布一致性\n",
    "    print(f\"\\n=== 分布一致性验证 ===\")\n",
    "    all_type_names = set(all_types)\n",
    "    \n",
    "    for type_name in sorted(all_type_names):\n",
    "        train_ratio = train_dist.get(type_name, 0) / len(train_texts) * 100\n",
    "        val_ratio = val_dist.get(type_name, 0) / len(val_texts) * 100\n",
    "        test_ratio = test_dist.get(type_name, 0) / len(test_texts) * 100\n",
    "        \n",
    "        print(f\"{type_name}:\")\n",
    "        print(f\"  训练集: {train_ratio:.2f}%, 验证集: {val_ratio:.2f}%, 测试集: {test_ratio:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'train': (train_texts, train_labels, train_types),\n",
    "        'val': (val_texts, val_labels, val_types),\n",
    "        'test': (test_texts, test_labels, test_types)\n",
    "    }\n",
    "\n",
    "def create_dataloaders(data_dict, tokenizer, batch_size=16):\n",
    "    \"\"\"创建训练、验证和测试集的DataLoader\"\"\"\n",
    "    print(\"正在创建数据加载器...\")\n",
    "    \n",
    "    train_dataset = AmbiguityDataset(\n",
    "        texts=data_dict['train'][0],\n",
    "        labels=data_dict['train'][1],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    val_dataset = AmbiguityDataset(\n",
    "        texts=data_dict['val'][0],\n",
    "        labels=data_dict['val'][1],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    test_dataset = AmbiguityDataset(\n",
    "        texts=data_dict['test'][0],\n",
    "        labels=data_dict['test'][1],\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs, patience=3):\n",
    "    \"\"\"训练模型并在每个epoch后评估\"\"\"\n",
    "    best_val_f1 = 0\n",
    "    counter = 0\n",
    "    best_model = None\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_accuracy': [],\n",
    "        'val_precision': [],\n",
    "        'val_recall': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"开始 Epoch {epoch+1}/{num_epochs}...\")\n",
    "        \n",
    "        # 训练\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=\"训练中\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1} 平均训练损失: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # 验证\n",
    "        val_metrics = evaluate_model(model, val_dataloader)\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = val_metrics\n",
    "        \n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['val_precision'].append(val_precision)\n",
    "        history['val_recall'].append(val_recall)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} 验证集评估:\")\n",
    "        print(f\"  准确率: {val_accuracy:.4f}\")\n",
    "        print(f\"  精确率: {val_precision:.4f}\")\n",
    "        print(f\"  召回率: {val_recall:.4f}\")\n",
    "        print(f\"  F1分数: {val_f1:.4f}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            counter = 0\n",
    "            best_model = model.state_dict().copy()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"早停激活，{patience} 个epoch没有提升\")\n",
    "                break\n",
    "    \n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            \n",
    "            predictions.extend(pred)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def predict_ambiguity(text, model, tokenizer):\n",
    "    \"\"\"预测句子是否有歧义\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    pred_class = np.argmax(probs)\n",
    "    \n",
    "    result = \"歧义句\" if pred_class == 1 else \"非歧义句\"\n",
    "    probability = probs[pred_class]\n",
    "    ambig_prob = probs[1]\n",
    "    non_ambig_prob = probs[0]\n",
    "    \n",
    "    return result, probability, ambig_prob, non_ambig_prob\n",
    "\n",
    "def predict_on_test_set(model, tokenizer, data_dict):\n",
    "    \"\"\"在测试集上进行预测并返回结果\"\"\"\n",
    "    print(\"正在对测试集进行预测...\")\n",
    "    \n",
    "    test_texts, test_labels, test_types = data_dict['test']\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    ambig_probs = []\n",
    "    non_ambig_probs = []\n",
    "    \n",
    "    for text in tqdm(test_texts, desc=\"预测中\"):\n",
    "        result, prob, ambig_prob, non_ambig_prob = predict_ambiguity(text, model, tokenizer)\n",
    "        predictions.append(result)\n",
    "        probabilities.append(prob)\n",
    "        ambig_probs.append(ambig_prob)\n",
    "        non_ambig_probs.append(non_ambig_prob)\n",
    "    \n",
    "    # 创建测试集DataFrame\n",
    "    test_df = pd.DataFrame({\n",
    "        'text': test_texts,\n",
    "        'true_label': ['歧义句' if label == 1 else '非歧义句' for label in test_labels],\n",
    "        'ambiguity_type': test_types,\n",
    "        'predicted_label': predictions,\n",
    "        'prediction_probability': probabilities,\n",
    "        'ambiguity_probability': ambig_probs,\n",
    "        'non_ambiguity_probability': non_ambig_probs\n",
    "    })\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "def save_datasets_as_tsv(data_dict, output_dir=\"./datasets\"):\n",
    "    \"\"\"将数据集保存为TSV文件\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    split_mapping = {\n",
    "        'train': 'train.tsv',\n",
    "        'val': 'dev.tsv',  # 验证集命名为dev.tsv\n",
    "        'test': 'test.tsv'\n",
    "    }\n",
    "    \n",
    "    for split_name, (texts, labels, types) in data_dict.items():\n",
    "        df = pd.DataFrame({\n",
    "            'text': texts,\n",
    "            'label': ['歧义句' if label == 1 else '非歧义句' for label in labels],\n",
    "            'ambiguity_type': types\n",
    "        })\n",
    "        \n",
    "        filename = split_mapping[split_name]\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        df.to_csv(output_path, sep='\\t', index=False, encoding='utf-8')\n",
    "        print(f\"已保存 {split_name} 数据集到: {output_path}\")\n",
    "\n",
    "def save_model(model, tokenizer, output_dir):\n",
    "    \"\"\"保存模型和分词器\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    print(f\"保存模型到 {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"加载保存的模型和分词器\"\"\"\n",
    "    print(f\"从 {model_path} 加载模型...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "def run_complete_pipeline(data_file, model_name, output_dir, num_epochs=10, batch_size=16, \n",
    "                         learning_rate=2e-5, weight_decay=0.01):\n",
    "    \"\"\"完整的训练流程\"\"\"\n",
    "    # 加载分词器\n",
    "    print(f\"加载分词器: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # 按要求准备数据\n",
    "    data_dict = prepare_data_with_requirements(data_file)\n",
    "    \n",
    "    # 5. 保存划分后的数据集为TSV文件\n",
    "    save_datasets_as_tsv(data_dict)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_dataloader, val_dataloader, test_dataloader = create_dataloaders(\n",
    "        data_dict, tokenizer, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # 加载模型\n",
    "    print(f\"加载模型: {model_name}\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=2\n",
    "    ).to(device)\n",
    "    \n",
    "    # 设置优化器和调度器\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"开始训练模型...\")\n",
    "    model, history = train_model(\n",
    "        model, train_dataloader, val_dataloader, optimizer, scheduler, num_epochs\n",
    "    )\n",
    "    \n",
    "    # 4. 在测试集上进行预测\n",
    "    test_predictions = predict_on_test_set(model, tokenizer, data_dict)\n",
    "    \n",
    "    # 保存测试集预测结果\n",
    "    test_predictions.to_csv(\"test_predictions.csv\", index=False, encoding='utf-8')\n",
    "    print(\"测试集预测结果已保存到: test_predictions.csv\")\n",
    "    \n",
    "    # 评估测试集性能\n",
    "    test_metrics = evaluate_model(model, test_dataloader)\n",
    "    print(\"测试集结果:\")\n",
    "    print(f\"  准确率: {test_metrics[0]:.4f}\")\n",
    "    print(f\"  精确率: {test_metrics[1]:.4f}\")\n",
    "    print(f\"  召回率: {test_metrics[2]:.4f}\")\n",
    "    print(f\"  F1分数: {test_metrics[3]:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    save_model(model, tokenizer, output_dir)\n",
    "    \n",
    "    return model, tokenizer, test_predictions\n",
    "\n",
    "# 主函数执行\n",
    "if __name__ == \"__main__\":\n",
    "    # 配置参数\n",
    "    model_name = \"hfl/chinese-roberta-wwm-ext\"\n",
    "    data_file = r'D:\\浏览器\\中文文本歧义收集与标注_数据表_歧义收集与标注 (2).csv'  # 修改为您的文件路径\n",
    "    output_dir = \"./ambiguity_detection_model_FINAL\"\n",
    "    \n",
    "    # 运行完整流程\n",
    "    model, tokenizer, test_predictions = run_complete_pipeline(\n",
    "        data_file, \n",
    "        model_name, \n",
    "        output_dir,\n",
    "        num_epochs=10,\n",
    "        batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    \n",
    "    print(\"完整流程执行完成！\")\n",
    "    print(\"生成的文件:\")\n",
    "    print(\"- train.tsv: 训练集\")\n",
    "    print(\"- dev.tsv: 验证集\")  \n",
    "    print(\"- test.tsv: 测试集\")\n",
    "    print(\"- test_predictions.csv: 测试集预测结果\")\n",
    "    print(f\"- {output_dir}: 训练好的模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19cf4007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  数量  比例（%）\n",
      "predicted_label            \n",
      "非歧义句             297  68.43\n",
      "歧义句              137  31.57\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 读取CSV文件\n",
    "file_path = r'D:\\python\\Coding\\NLP\\Classifier\\模型训练\\Classifier\\test_predictions.csv'  # 请根据实际路径修改\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. 统计 predicted_label 列中歧义句和非歧义句的数量和比例\n",
    "label_counts = df['predicted_label'].value_counts()\n",
    "label_ratios = df['predicted_label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# 3. 整合统计结果为DataFrame\n",
    "label_summary = pd.DataFrame({\n",
    "    '数量': label_counts,\n",
    "    '比例（%）': label_ratios.round(2)\n",
    "})\n",
    "\n",
    "# 4. 输出结果\n",
    "print(label_summary)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
